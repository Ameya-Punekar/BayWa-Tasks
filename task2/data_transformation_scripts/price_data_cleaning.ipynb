{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPVk72sznJEX",
        "outputId": "2c490d84-e66f-4a83-da34-cf436cc6c3e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,071 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,425 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,601 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,654 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Fetched 16.6 MB in 5s (3,588 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.1 delta-spark==3.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNo63JmanL5P",
        "outputId": "e36bf362-0252-4467-9867-fa88907f4dea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.1\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting delta-spark==3.2.0\n",
            "  Downloading delta_spark-3.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark==3.2.0) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark==3.2.0) (3.20.2)\n",
            "Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488492 sha256=f0c55b4cce5d38a46fb7a3855e9015881bc4805b305b093d09a1809b82b22522\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark, delta-spark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.3\n",
            "    Uninstalling pyspark-3.5.3:\n",
            "      Successfully uninstalled pyspark-3.5.3\n",
            "Successfully installed delta-spark-3.2.0 pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "metadata": {
        "id": "Z9pVCQFAnL3f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session with Delta Lake support and required configurations\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"Price Data Cleaning\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\")\n",
        "    .config(\"spark.sql.crossJoin.enabled\", \"true\")\n",
        "    .getOrCreate()\n",
        ")\n"
      ],
      "metadata": {
        "id": "BXPYfOZonL1P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the price data from a JSON file\n",
        "price_data_df = spark.read.option(\"multiline\", \"true\").json(\"/content/price_data.json\")\n",
        "\n",
        "print(\"Initial DataFrame count:\", price_data_df.count())\n",
        "price_data_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA1c6iUSnLzI",
        "outputId": "b8a6ac09-220c-47ad-d277-6882f9cfeada"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame count: 1\n",
            "+----------+---------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|deprecated|license_info                                                                     |price                                                                                                                                                                            |unit               |unix_seconds                                                                                                                                                                                                                                                                                    |\n",
            "+----------+---------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|false     |CC BY 4.0 (creativecommons.org/licenses/by/4.0) from Bundesnetzagentur | SMARD.de|[115.29, 109.64, 106.95, 107.79, 105.7, 106.82, 113.93, 125.71, 126.0, 117.57, 101.39, 81.48, 68.35, 63.3, 64.55, 81.44, 100.6, 125.73, 150.9, 118.74, 96.48, 88.76, 87.5, 76.63]|EUR / megawatt_hour|[1729893600, 1729897200, 1729900800, 1729904400, 1729908000, 1729911600, 1729915200, 1729918800, 1729922400, 1729926000, 1729929600, 1729933200, 1729936800, 1729940400, 1729944000, 1729947600, 1729951200, 1729954800, 1729958400, 1729962000, 1729965600, 1729969200, 1729972800, 1729976400]|\n",
            "+----------+---------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the data\n",
        "# Filter out deprecated entries and rows with null values in 'price', 'unit', or 'unix_seconds'\n",
        "from pyspark.sql.functions import col, from_unixtime, explode, posexplode\n",
        "# Clean the data\n",
        "# Filter out deprecated entries and rows with null values in 'price', 'unit', or 'unix_seconds'\n",
        "price_cleaned = price_data_df.filter(\n",
        "    (col(\"deprecated\") == False) &\n",
        "    (col(\"price\").isNotNull()) &\n",
        "    (col(\"unit\").isNotNull()) &\n",
        "    (col(\"unix_seconds\").isNotNull())\n",
        ")\n",
        "\n",
        "# Use posexplode to maintain the relationship between unix_seconds and price\n",
        "price_exploded = price_cleaned.select(\n",
        "    \"license_info\",\n",
        "    posexplode(col(\"unix_seconds\")).alias(\"pos\", \"unix_seconds\"),\n",
        "    col(\"price\"),\n",
        "    \"unit\",\n",
        "    \"deprecated\"\n",
        ")\n",
        "\n",
        "# Select the corresponding price using the position index\n",
        "price_exploded = price_exploded.select(\n",
        "    \"license_info\",\n",
        "    col(\"unix_seconds\"),\n",
        "    col(\"price\")[col(\"pos\")].alias(\"price\"),  # Match price with the same position\n",
        "    \"unit\",\n",
        "    \"deprecated\"\n",
        ")\n",
        "\n",
        "# Convert Unix timestamps to a readable timestamp format\n",
        "price_exploded = price_exploded.withColumn(\"timestamp\", from_unixtime(col(\"unix_seconds\")))\n",
        "\n",
        "# Select relevant columns\n",
        "price_cleaned_final = price_exploded.select(\n",
        "    \"license_info\",\n",
        "    \"price\",\n",
        "    \"unit\",\n",
        "    \"deprecated\",\n",
        "    \"timestamp\"\n",
        ")\n",
        "\n",
        "# Specify the path where the cleaned CSV file will be stored\n",
        "csv_file_path = \"/content/price_cleaned.csv\"\n",
        "\n",
        "# Save the cleaned DataFrame as a CSV file\n",
        "price_cleaned_final.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(csv_file_path)\n",
        "\n",
        "print(f\"Cleaned price data CSV file saved at: {csv_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqL5asLcnLw3",
        "outputId": "bebd63e8-32c3-4328-a15e-bdf4d77094f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned price data CSV file saved at: /content/price_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Prg8WUlpnLux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zUhwHf0DnLse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6lLi90QmnLqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMMPg6LBnLoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-acDPrnnLmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OstCCDnLnLj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKkwZ0PMnLfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQcQlqGJnLdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Roafm4kenLbH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}